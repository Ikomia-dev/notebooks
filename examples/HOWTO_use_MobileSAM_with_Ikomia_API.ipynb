{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/Ikomia-dev/notebooks/blob/main/examples/img/banner_ikomia.png?raw=true'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run MobileSAM with the Ikomia API "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MobileSAM** (Faster Segment Anything) is a streamlined and efficient variant of the Segment Anything Model (SAM), optimized for mobile applications. \n",
    "\n",
    "The innovation primarily addresses the challenge posed by the original SAM's resource-intensive image encoder. MobileSAM introduces a lightweight image encoder, significantly reducing the model's size and computational demands without compromising performance.\n",
    "\n",
    "![illustration](https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/mask_point.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install Ikomia Python API with pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ikomia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**-Google Colab ONLY- Restart runtime**\n",
    "\n",
    "Click on the \"RESTART RUNTIME\" button at the end the previous window.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MobileSAM on your image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikomia.dataprocess.workflow import Workflow\n",
    "\n",
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add algorithm\n",
    "algo  = wf.add_task(name = \"infer_mobile_segment_anything\", auto_connect=True)\n",
    "\n",
    "# Setting parameters: boxes on the wheels\n",
    "algo.set_parameters({\n",
    "    \"input_box\": \"[[425, 600, 700, 875], [1240, 675, 1400, 750], [1375, 550, 1650, 800]]\"\n",
    "})\n",
    "\n",
    "# Run directly on your image\n",
    "wf.run_on(url=\"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/images/truck.jpg?raw=true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikomia.utils.displayIO import display\n",
    "\n",
    "# Display segmentation mask\n",
    "from PIL import ImageShow\n",
    "ImageShow.register(ImageShow.IPythonViewer(), 0)\n",
    "\n",
    "display(algo.get_image_with_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point prompt (select mask output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add algorithm\n",
    "algo  = wf.add_task(name = \"infer_mobile_segment_anything\", auto_connect=True)\n",
    "\n",
    "# Setting parameters: boxes on the wheels\n",
    "algo.set_parameters({\n",
    "    \"input_point\": \"[500, 375]\",\n",
    "    \"mask_id\":\"1\"\n",
    "})\n",
    "\n",
    "# Run directly on your image\n",
    "wf.run_on(url=\"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/images/truck.jpg?raw=true\")\n",
    "\n",
    "# Display your image\n",
    "display(algo.get_image_with_mask())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add algorithm\n",
    "algo  = wf.add_task(name = \"infer_mobile_segment_anything\", auto_connect=True)\n",
    "\n",
    "# Setting parameters: boxes on the wheels\n",
    "algo.set_parameters({\n",
    "    \"input_point\": \"[500, 375]\",\n",
    "    \"mask_id\":\"3\"\n",
    "})\n",
    "\n",
    "# Run directly on your image\n",
    "wf.run_on(url=\"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/images/truck.jpg?raw=true\")\n",
    "\n",
    "# Display your image\n",
    "display(algo.get_image_with_mask())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add algorithm\n",
    "algo  = wf.add_task(name = \"infer_mobile_segment_anything\", auto_connect=True)\n",
    "\n",
    "# Setting parameters: boxes on the wheels\n",
    "algo.set_parameters({\n",
    "    \"input_box\": \"[425, 600, 700, 875]\",\n",
    "    \"input_point\": \"[500, 375]\",\n",
    "    \"input_point_label\": \"0\"\n",
    "})\n",
    "\n",
    "# Run directly on your image\n",
    "wf.run_on(url=\"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/images/truck.jpg?raw=true\")\n",
    "\n",
    "# Display your image\n",
    "display(algo.get_image_with_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic mask generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add algorithm\n",
    "algo  = wf.add_task(name = \"infer_mobile_segment_anything\", auto_connect=True)\n",
    "\n",
    "# Setting parameters: boxes on the wheels\n",
    "algo.set_parameters({\n",
    "    \"points_per_side\": \"16\",\n",
    "})\n",
    "\n",
    "# Run directly on your image\n",
    "wf.run_on(url=\"https://github.com/Ikomia-dev/notebooks/blob/main/examples/img/img_work.jpg?raw=true\")\n",
    "\n",
    "# Display your image\n",
    "display(algo.get_image_with_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of parameters\n",
    "\n",
    "- **input_box** (list): A Nx4 array of given box prompts to the  model, in [XYXY] or [[XYXY], [XYXY]] format.\n",
    "- **draw_graphic_input** (Boolean): When set to True, it allows you to draw graphics (box or point) over the object you wish to segment. If set to False, MobileSAM will automatically generate masks for the entire image.\n",
    "- **mask_id** (int) - default '1': When [a single graphic point](https://github.com/Ikomia-hub/infer_mobile_segment_anything#a-single-point) is selected, MobileSAM with generate three outputs given a single point (3 best scores). You can select which mask to output using the mask_id parameters (1, 2 or 3). \n",
    "- **input_point** (list, *optional*): A Nx2 array of point prompts to the model. Each point is in [X,Y] in pixels.\n",
    "- **input_point_label** (list, *optional*): A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point\n",
    "- **points_per_side** (int) - default '32' : (Automatic detection mode). The number of points to be sampled along one side of the image. The total number of points is points_per_side**2. \n",
    "- **points_per_batch** (int) - default '64': (Automatic detection mode).  Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.\n",
    "- **stability_score_thres** (float) - default '0.95': Filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.\n",
    "- **box_nms_thres** (float) - default '0.7': The box IoU cutoff used by non-maximal suppression to filter duplicate masks.\n",
    "- **iou_thres** (float) - default '0.88': A filtering threshold in [0,1], using the model's predicted mask quality.\n",
    "- **crop_n_layers** (int) - default '0' : If >0, mask prediction will be run again oncrops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.\n",
    "- **crop_nms_thres** (float) - default '0': The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.\n",
    "- **crop_overlap_ratio** (float) default 'float(512 / 1500)'\n",
    "- **crop_n_points_downscale_factor** (int) - default '1' : The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n",
    "- **min_mask_region_area** (int) - default '0': op layer. Exclusive with points_per_side. min_mask_region_area (int): If >0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than min_mask_region_area. \n",
    "- **input_size_percent** (int) - default '100': Percentage size of the input image. Can be reduce to save memory usage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
